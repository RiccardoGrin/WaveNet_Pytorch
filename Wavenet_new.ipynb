{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from graphviz import Digraph\n",
    "import re\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Conv1d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n",
    "        super(CausalConv1d, self).__init__(in_channels, out_channels, kernel_size, stride=stride, padding=0,\n",
    "            dilation=dilation, groups=groups, bias=bias)\n",
    "\n",
    "        self.left_padding = dilation * (kernel_size - 1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        temp = torch.unsqueeze(input, 0)\n",
    "        x = F.pad(temp, (self.left_padding, 0, 0, 0))\n",
    "        x = torch.squeeze(x, 0)\n",
    "        \n",
    "        return super(CausalConv1d, self).forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OneHot(nn.Module):\n",
    "    def __init__(self, quant):\n",
    "        super(OneHot, self).__init__()\n",
    "        self.one = torch.sparse.torch.eye(quant)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return Variable(self.one.index_select(0, input.data)).unsqueeze(0).transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " (0 ,.,.) = \n",
       "    0   0   0   0   0   0   0   0   0   0\n",
       "    0   0   0   0   1   1   0   0   0   0\n",
       "    0   0   0   0   0   0   0   0   0   1\n",
       "    0   1   1   0   0   0   1   0   0   0\n",
       "    1   0   0   1   0   0   0   1   1   0\n",
       " [torch.FloatTensor of size 1x5x10], Variable containing:\n",
       "  4\n",
       "  3\n",
       "  3\n",
       "  4\n",
       "  1\n",
       "  1\n",
       "  3\n",
       "  4\n",
       "  4\n",
       "  2\n",
       " [torch.LongTensor of size 10])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Variable(torch.LongTensor(np.random.randint(1,5,10)))\n",
    "\n",
    "onehot = OneHot(5)\n",
    "onehot(data), data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, quant = 256, res_size = 512, skip_size = 256, dilation_layers = 10, stacks = 3):\n",
    "        super(WaveNet, self).__init__()\n",
    "        self.dilation_layers = dilation_layers\n",
    "        self.dilations = dilations = [(2**dilation) for dilation in range(dilation_layers)] * stacks\n",
    "        self.one_hot = OneHot(quant)\n",
    "        \n",
    "        self.causal_conv = CausalConv1d(quant, res_size, 1)\n",
    "        \n",
    "        self.dial_tanh_conv = nn.ModuleList([CausalConv1d(skip_size, skip_size, 2, dilation = d) for d in dilations])\n",
    "        self.dial_sigm_conv = nn.ModuleList([CausalConv1d(skip_size, skip_size, 2, dilation = d) for d in dilations])\n",
    "        \n",
    "        self.dial_skip_conv = nn.ModuleList([CausalConv1d(skip_size, skip_size, 1) for _ in dilations])\n",
    "        self.dial_res_conv = nn.ModuleList([CausalConv1d(skip_size, res_size, 1) for _ in dilations])\n",
    "        \n",
    "        self.end_conv1 = nn.Conv1d(in_channels = skip_size, out_channels = skip_size, kernel_size = 1)\n",
    "        self.end_conv2 = nn.Conv1d(in_channels = skip_size, out_channels = quant, kernel_size = 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.one_hot(input)\n",
    "        output = self.causal_conv(output)\n",
    "        \n",
    "        skip_sum = []\n",
    "        for s, t, skip_conv, res_conv in zip(self.dial_sigm_conv, self.dial_tanh_conv, self.dial_skip_conv, self.dial_res_conv):\n",
    "            res_output = output\n",
    "            \n",
    "            gate_output = self.gated_unit(res_output, s, t)\n",
    "            \n",
    "            output = res_conv(gate_output)\n",
    "            output = output + res_output[:,:,-output.size(2):]\n",
    "            \n",
    "            skip = skip_conv(gate_output)\n",
    "            skip_sum.append(skip)\n",
    "        \n",
    "        output = sum([s[:,:,-output.size(2):] for s in skip_sum])\n",
    "        \n",
    "        output = self.postprocess(output)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def gated_unit(self, input, dial_sigm_conv, dial_tanh_conv):\n",
    "        input_sigmoid = input[0][256:].unsqueeze(0)\n",
    "        input_tanh = input[0][:256].unsqueeze(0)\n",
    "        \n",
    "        output_sigmoid = dial_sigm_conv(input_sigmoid)\n",
    "        output_tanh = dial_tanh_conv(input_tanh)\n",
    "        \n",
    "        output = nn.functional.sigmoid(output_sigmoid) * nn.functional.tanh(output_tanh)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def postprocess(self, input):\n",
    "        output = nn.functional.elu(input)\n",
    "        output = self.end_conv1(output)\n",
    "        output = nn.functional.elu(output)\n",
    "        output = self.end_conv2(output).squeeze(0).transpose(0,1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = WaveNet()\n",
    "batch = Variable(torch.from_numpy(np.random.randint(0,256,10000).astype(np.long)))\n",
    "net(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0  ,.,.) = \n",
       "  5.6435e-01  5.4275e-01  7.0707e-01  ...   4.9807e-01  7.1019e-01  2.9433e-01\n",
       "  4.2612e-01  1.9716e-01  9.5337e-01  ...   3.3937e-01  8.2537e-01  3.3836e-02\n",
       "  3.1062e-01  6.4660e-01  7.5954e-02  ...   6.8057e-01  6.8052e-01  1.0355e-01\n",
       "                 ...                   ⋱                   ...                \n",
       "  9.4133e-01  8.9128e-01  8.8054e-01  ...   8.1223e-01  1.2172e-01  6.8401e-01\n",
       "  8.3522e-01  3.3828e-01  8.3925e-01  ...   1.5371e-01  2.7752e-01  7.9269e-01\n",
       "  9.2236e-01  2.5876e-01  3.6675e-04  ...   2.6285e-01  5.5589e-01  2.4504e-02\n",
       "[torch.FloatTensor of size 1x512x5000]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Variable(torch.rand([1,512,5000])); data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0  ,.,.) = \n",
       "  8.1559e-01  7.2862e-01  8.2748e-01  ...   4.8296e-01  3.3091e-01  5.1204e-01\n",
       "  3.7281e-01  9.1877e-01  3.3286e-01  ...   4.6933e-01  1.8827e-01  1.0883e-01\n",
       "  7.5145e-01  4.0418e-01  4.2649e-01  ...   4.6620e-01  8.4367e-01  1.5885e-01\n",
       "                 ...                   ⋱                   ...                \n",
       "  9.4133e-01  8.9128e-01  8.8054e-01  ...   8.1223e-01  1.2172e-01  6.8401e-01\n",
       "  8.3522e-01  3.3828e-01  8.3925e-01  ...   1.5371e-01  2.7752e-01  7.9269e-01\n",
       "  9.2236e-01  2.5876e-01  3.6675e-04  ...   2.6285e-01  5.5589e-01  2.4504e-02\n",
       "[torch.FloatTensor of size 1x256x5000]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = data[0][256:]; data1.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0  ,.,.) = \n",
       "  5.6435e-01  5.4275e-01  7.0707e-01  ...   4.9807e-01  7.1019e-01  2.9433e-01\n",
       "  4.2612e-01  1.9716e-01  9.5337e-01  ...   3.3937e-01  8.2537e-01  3.3836e-02\n",
       "  3.1062e-01  6.4660e-01  7.5954e-02  ...   6.8057e-01  6.8052e-01  1.0355e-01\n",
       "                 ...                   ⋱                   ...                \n",
       "  3.6317e-01  1.2844e-01  1.7922e-01  ...   6.7068e-01  6.4877e-01  9.3521e-02\n",
       "  9.7565e-02  6.3691e-01  3.1665e-01  ...   8.2483e-01  2.6525e-01  2.0491e-01\n",
       "  8.3155e-01  6.1810e-01  8.8408e-01  ...   5.6680e-02  2.1012e-01  1.1484e-01\n",
       "[torch.FloatTensor of size 1x256x5000]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = data[0][:256]; data2.unsqueeze(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
