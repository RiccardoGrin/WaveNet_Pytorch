{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from time import strftime, localtime\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Conv1d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n",
    "        super(CausalConv1d, self).__init__(in_channels, out_channels, kernel_size, stride=stride, padding=0,\n",
    "            dilation=dilation, groups=groups, bias=bias)\n",
    "\n",
    "        self.left_padding = dilation * (kernel_size - 1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        temp = torch.unsqueeze(input, 0)\n",
    "        x = F.pad(temp, (self.left_padding, 0, 0, 0))\n",
    "        x = torch.squeeze(x, 0)\n",
    "        \n",
    "        return super(CausalConv1d, self).forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OneHot(nn.Module):\n",
    "    def __init__(self, quant):\n",
    "        super(OneHot, self).__init__()\n",
    "        self.one = torch.sparse.torch.eye(quant)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return Variable(self.one.index_select(0, input.data)).unsqueeze(0).transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, quant = 256, res_size = 512, skip_size = 256, dilation_layers = 10, stacks = 3):\n",
    "        super(WaveNet, self).__init__()\n",
    "        self.dilation_layers = dilation_layers\n",
    "        self.dilations = dilations = [(2**dilation) for dilation in range(dilation_layers)] * stacks\n",
    "        self.one_hot = OneHot(quant)\n",
    "        \n",
    "        self.causal_conv = CausalConv1d(quant, res_size, 1)\n",
    "        \n",
    "        self.dial_conv = nn.ModuleList([CausalConv1d(res_size, res_size, 2, dilation = d) for d in dilations])\n",
    " \n",
    "        self.dial_skip_conv = nn.ModuleList([CausalConv1d(skip_size, skip_size, 1) for _ in dilations])\n",
    "        self.dial_res_conv = nn.ModuleList([CausalConv1d(skip_size, res_size, 1) for _ in dilations])\n",
    "        \n",
    "        self.end_conv1 = nn.Conv1d(in_channels = skip_size, out_channels = skip_size, kernel_size = 1)\n",
    "        self.end_conv2 = nn.Conv1d(in_channels = skip_size, out_channels = quant, kernel_size = 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.one_hot(input)\n",
    "        output = self.causal_conv(output)\n",
    "        \n",
    "        skip_sum = []\n",
    "        for d_conv, skip_conv, res_conv in zip(self.dial_conv, self.dial_skip_conv, self.dial_res_conv):\n",
    "            res_output = output\n",
    "            \n",
    "            gate_output = self.gated_unit(res_output, d_conv)\n",
    "            \n",
    "            output = res_conv(gate_output)\n",
    "            output = output + res_output[:,:,-output.size(2):]\n",
    "            \n",
    "            skip = skip_conv(gate_output)\n",
    "            skip_sum.append(skip)\n",
    "        \n",
    "        output = sum([s[:,:,-output.size(2):] for s in skip_sum])\n",
    "        \n",
    "        output = self.postprocess(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def gated_unit(self, input, dial_conv):\n",
    "        output = dial_conv(input)\n",
    "        \n",
    "        output_sigmoid = output[0][256:].unsqueeze(0)\n",
    "        output_tanh = output[0][:256].unsqueeze(0)\n",
    "\n",
    "        output = F.sigmoid(output_sigmoid) * F.tanh(output_tanh)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def postprocess(self, input):\n",
    "        output = F.relu(input)\n",
    "        output = self.end_conv1(output)\n",
    "        output = F.relu(output)\n",
    "        output = self.end_conv2(output).squeeze(0).transpose(0,1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mu Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mu_encoder(input):        \n",
    "    mu = torch.FloatTensor([255])\n",
    "    x_mu = torch.sign(input) * torch.log1p(mu * torch.abs(input)) / torch.log1p(mu)\n",
    "    x_mu = ((x_mu + 1) / 2 * mu + 0.5).long()\n",
    "    \n",
    "    return x_mu.type(torch.FloatTensor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mu_decoder(input):\n",
    "    mu = torch.FloatTensor([255])\n",
    "    x = ((input) / mu) * 2 - 1.\n",
    "    x = torch.sign(x) * (torch.exp(torch.abs(x) * torch.log1p(mu)) - 1.) / mu\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator():\n",
    "    while True:\n",
    "        xs = np.linspace(0, np.pi*10, 4000)\n",
    "        ys = np.sin(xs)\n",
    "        ys = ys.astype(np.float32)\n",
    "        x = torch.from_numpy(ys).view(1, 1, -1)\n",
    "        \n",
    "        if (torch.cuda.is_available() and CUDA):\n",
    "            x = x.cuda()\n",
    "            \n",
    "        yield x\n",
    "\n",
    "g = batch_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = next(g)\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(Variable(x).cpu().data.numpy()[0][0],'.', ms=3)\n",
    "\n",
    "temp = mu_encoder(x)\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(Variable(temp).cpu().data.numpy()[0][0],'.', ms=3)\n",
    "\n",
    "temp = mu_decoder(temp)\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(Variable(temp).cpu().data.numpy()[0][0],'.', ms=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load / Save functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename):\n",
    "    torch.save(state, filename)\n",
    "    print(\"Checkpoint saved: \" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(filename):\n",
    "    net = WaveNet()\n",
    "\n",
    "    checkpoint = torch.load(filename)\n",
    "    net.load_state_dict(checkpoint['state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss_save = checkpoint['loss_save']\n",
    "    optimizer = optim.Adam(net.parameters(),lr=0.001)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    \n",
    "    print(\"Loaded checkpoint: \" + filename)\n",
    "    return net, epoch, loss_save, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    load_checkpoint(\"filename\")\n",
    "except NameError:\n",
    "    print(\"No checkpoint found - Initializing new training\")\n",
    "    net = WaveNet()\n",
    "    loss_save = []\n",
    "    epoch = 0\n",
    "    optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "batch_size = 16\n",
    "max_epoch = 200\n",
    "\n",
    "for _ in range(max_epoch):\n",
    "    epoch += 1\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    for ind in range(batch_size):\n",
    "        batch = next(g)\n",
    "        batch = Variable(mu_encoder(batch).type(torch.LongTensor))\n",
    "        x = batch[0][0][:-1]\n",
    "        logits = net(x)\n",
    "        sz = logits.size(0)\n",
    "        loss = loss + nn.functional.cross_entropy(logits, batch[0][0][-sz:])\n",
    "        print(\"Batch \" + str(ind) + \" done\")\n",
    "    loss = loss/batch_size\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_save.append(loss.data[0])\n",
    "    \n",
    "    # monitor progress\n",
    "    if epoch%1 == 0:\n",
    "        print('epoch {}, loss {}'.format(epoch, loss.data[0]))\n",
    "        batch = next(g)\n",
    "        batch = Variable(mu_encoder(batch).type(torch.LongTensor))\n",
    "        x = batch[0][0][:-1]\n",
    "        logits = net(x)\n",
    "        _, i = logits.max(dim=1)\n",
    "        plt.figure(figsize=[16,4])\n",
    "        plt.plot(i.data.tolist())\n",
    "        plt.plot(x.data.tolist(),'.',ms=1)\n",
    "        plt.title('epoch {}'.format(epoch))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        cp = \"checkpoint_epoch_\" + str(epoch) + \"_\" + strftime(\"%Y_%m_%d__%H_%M\", localtime()) + '.pth')\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "            'loss_save' : loss_save,\n",
    "        }, cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = next(g)\n",
    "x = mu_encoder(batch).type(torch.LongTensor)\n",
    "n = 1000\n",
    "predict_save = []\n",
    "\n",
    "for _ in range(n):\n",
    "    temp = Variable(x)\n",
    "    inputs = temp[0][0][:-1]\n",
    "    predict = net(inputs)\n",
    "    _, i = predict.max(dim=1)\n",
    "    predict_save.append(i.data[-1])\n",
    "    x = np.roll(x, -1, axis=2)\n",
    "    x[0][0][-1] = i.data[-1]\n",
    "    x = torch.from_numpy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(predict_save)\n",
    "plt.xlim([0, 1000])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
