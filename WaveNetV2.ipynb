{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from graphviz import Digraph\n",
    "import re\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Conv1d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n",
    "        super(CausalConv1d, self).__init__(in_channels, out_channels, kernel_size, stride=stride, padding=0,\n",
    "            dilation=dilation, groups=groups, bias=bias)\n",
    "\n",
    "        self.left_padding = dilation * (kernel_size - 1)\n",
    "#         self.weight = init.xavier_uniform(self.weight, gain=np.sqrt(2))\n",
    "#         self.bias = init.constant(self.bias, 0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        temp = torch.unsqueeze(input, 0)\n",
    "        x = F.pad(temp, (self.left_padding, 0, 0, 0))\n",
    "        x = torch.squeeze(x, 0)\n",
    "        \n",
    "        return super(CausalConv1d, self).forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = CausalConv1d(1, 1, 2, dilation = 512)\n",
    "        self.conv2 = CausalConv1d(1, 1, 2, dilation = 256)\n",
    "        self.conv3 = CausalConv1d(1, 1, 2, dilation = 128)\n",
    "        self.conv4 = CausalConv1d(1, 1, 2, dilation = 64)\n",
    "        self.conv5 = CausalConv1d(1, 1, 2, dilation = 32)\n",
    "        self.conv6 = CausalConv1d(1, 1, 2, dilation = 16)\n",
    "        self.conv7 = CausalConv1d(1, 1, 2, dilation = 8)\n",
    "        self.conv8 = CausalConv1d(1, 1, 2, dilation = 4)\n",
    "        self.conv9 = CausalConv1d(1, 1, 2, dilation = 2)\n",
    "        self.conv10 = CausalConv1d(1, 1, 2, dilation = 1)\n",
    "        self.lin1 = nn.Tanh()\n",
    "#         self.lin1 = nn.ReLU()\n",
    "        self.oneone = nn.Conv1d(1, 1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv10(x)\n",
    "        x = self.lin1(x)\n",
    "#         y = self.lin2(x)\n",
    "        x = self.conv9(x)\n",
    "        x = self.lin1(x)\n",
    "#         y = self.lin2(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.lin1(x)\n",
    "#         y = self.lin2(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.lin1(x)\n",
    "#         y = self.lin2(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.lin1(x)\n",
    "#         y = self.lin2(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.lin1(x)\n",
    "#         y = self.lin2(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.lin1(x)\n",
    "        x = self.oneone(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv10 = CausalConv1d(1, 1, 2, dilation = 512)\n",
    "        self.conv9 = CausalConv1d(1, 1, 2, dilation = 256)\n",
    "        self.conv8 = CausalConv1d(1, 1, 2, dilation = 128)\n",
    "        self.conv7 = CausalConv1d(1, 1, 2, dilation = 64)\n",
    "        self.conv6 = CausalConv1d(1, 1, 2, dilation = 32)\n",
    "        self.conv5 = CausalConv1d(1, 1, 2, dilation = 16)\n",
    "        self.conv4 = CausalConv1d(1, 1, 2, dilation = 8)\n",
    "        self.conv3 = CausalConv1d(1, 1, 2, dilation = 4)\n",
    "        self.conv2 = CausalConv1d(1, 1, 2, dilation = 2)\n",
    "        self.conv1 = CausalConv1d(1, 1, 2, dilation = 1)\n",
    "        self.Tanh = nn.Tanh()\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.oneone = nn.Conv1d(1, 1, 1)\n",
    "        self.LogSoftmax = nn.LogSoftmax(1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.conv9(x)\n",
    "        x = self.oneone(x)\n",
    "\n",
    "#         x5 = self.conv5(resAdd3)\n",
    "#         t5 = self.Tanh(x5)\n",
    "#         s5 = self.Sigmoid(x5)\n",
    "#         oneone5 = self.oneone(t5 * s5)\n",
    "#         resAdd4 = oneone5 + oneone4\n",
    "         \n",
    "#         x6 = self.conv6(resAdd4)\n",
    "#         t6 = self.Tanh(x6)\n",
    "#         s6 = self.Sigmoid(x6)\n",
    "#         oneone6 = self.oneone(t6 * s6)\n",
    "#         resAdd5 = oneone6 + oneone5\n",
    "       \n",
    "#         x7 = self.conv7(resAdd5)\n",
    "#         t7 = self.Tanh(x7)\n",
    "#         s7 = self.Sigmoid(x7)\n",
    "#         oneone7 = self.oneone(t7 * s7)\n",
    "#         resAdd6 = oneone7 + oneone6\n",
    "        \n",
    "#         x8 = self.conv8(resAdd6)\n",
    "#         t8 = self.Tanh(x8)\n",
    "#         s8 = self.Sigmoid(x8)\n",
    "#         oneone8 = self.oneone(t8 * s8)\n",
    "#         resAdd7 = oneone8 + oneone7\n",
    "        \n",
    "#         x9 = self.conv9(resAdd7)\n",
    "#         t9 = self.Tanh(x9)\n",
    "#         s9 = self.Sigmoid(x9)\n",
    "#         oneone9 = self.oneone(t9 * s9)\n",
    "#         resAdd8 = oneone9 + oneone8\n",
    "        \n",
    "#         x10 = self.conv10(resAdd8)\n",
    "#         t10 = self.Tanh(x10)\n",
    "#         s10 = self.Sigmoid(x10)\n",
    "#         oneone10 = self.oneone(t10 * s10)\n",
    "#         resAdd9 = oneone10 + oneone9\n",
    "        \n",
    "        #x = oneone2 + oneone3 + oneone4 #+ oneone5 + oneone6 + oneone7 + oneone8 + oneone9 + oneone10\n",
    "\n",
    "#         x = self.ReLU(x)\n",
    "#         x = self.oneone(x)\n",
    "#         x = self.ReLU(x)\n",
    "#         x = self.oneone(x)\n",
    "#         x = self.LogSoftmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv10 = CausalConv1d(1, 1, 2, dilation = 512)\n",
    "        self.conv9 = CausalConv1d(1, 1, 2, dilation = 256)\n",
    "        self.conv8 = CausalConv1d(1, 1, 2, dilation = 128)\n",
    "        self.conv7 = CausalConv1d(1, 1, 2, dilation = 64)\n",
    "        self.conv6 = CausalConv1d(1, 1, 2, dilation = 32)\n",
    "        self.conv5 = CausalConv1d(1, 1, 2, dilation = 16)\n",
    "        self.conv4 = CausalConv1d(1, 1, 2, dilation = 8)\n",
    "        self.conv3 = CausalConv1d(1, 1, 2, dilation = 4)\n",
    "        self.conv2 = CausalConv1d(1, 1, 2, dilation = 2)\n",
    "        self.conv1 = CausalConv1d(1, 1, 2, dilation = 1)\n",
    "        self.Tanh = nn.Tanh()\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.oneone = nn.Conv1d(1, 1, 1)\n",
    "        self.LogSoftmax = nn.LogSoftmax(1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "      \n",
    "        x2 = self.conv2(x1)\n",
    "        t2 = self.Tanh(x2)\n",
    "        s2 = self.Sigmoid(x2)\n",
    "        oneone2 = self.oneone(t2 * s2) \n",
    "        resAdd1 = oneone2 + x1\n",
    "        \n",
    "        x3 = self.conv3(resAdd1)\n",
    "        t3 = self.Tanh(x3)\n",
    "        s3 = self.Sigmoid(x3)\n",
    "        oneone3 = self.oneone(t3 * s3)\n",
    "        resAdd2 = oneone3 + oneone2\n",
    "        \n",
    "        x4 = self.conv4(resAdd2)\n",
    "        t4 = self.Tanh(x4)\n",
    "        s4 = self.Sigmoid(x4)\n",
    "        oneone4 = self.oneone(t4 * s4)\n",
    "        resAdd3 = oneone4 + oneone3\n",
    "        \n",
    "        x5 = self.conv5(resAdd3)\n",
    "        t5 = self.Tanh(x5)\n",
    "        s5 = self.Sigmoid(x5)\n",
    "        oneone5 = self.oneone(t5 * s5)\n",
    "        resAdd4 = oneone5 + oneone4\n",
    "         \n",
    "        x6 = self.conv6(resAdd4)\n",
    "        t6 = self.Tanh(x6)\n",
    "        s6 = self.Sigmoid(x6)\n",
    "        oneone6 = self.oneone(t6 * s6)\n",
    "        resAdd5 = oneone6 + oneone5\n",
    "       \n",
    "        x7 = self.conv7(resAdd5)\n",
    "        t7 = self.Tanh(x7)\n",
    "        s7 = self.Sigmoid(x7)\n",
    "        oneone7 = self.oneone(t7 * s7)\n",
    "        resAdd6 = oneone7 + oneone6\n",
    "        \n",
    "        x8 = self.conv8(resAdd6)\n",
    "        t8 = self.Tanh(x8)\n",
    "        s8 = self.Sigmoid(x8)\n",
    "        oneone8 = self.oneone(t8 * s8)\n",
    "        resAdd7 = oneone8 + oneone7\n",
    "        \n",
    "        x9 = self.conv9(resAdd7)\n",
    "        t9 = self.Tanh(x9)\n",
    "        s9 = self.Sigmoid(x9)\n",
    "        oneone9 = self.oneone(t9 * s9)\n",
    "        resAdd8 = oneone9 + oneone8\n",
    "        \n",
    "        x10 = self.conv10(resAdd8)\n",
    "        t10 = self.Tanh(x10)\n",
    "        s10 = self.Sigmoid(x10)\n",
    "        oneone10 = self.oneone(t10 * s10)\n",
    "        resAdd9 = oneone10 + oneone9\n",
    "        \n",
    "        x = oneone2 + oneone3 + oneone4 + oneone5 + oneone6 + oneone7 + oneone8 + oneone9 + oneone10\n",
    "\n",
    "        x = self.ReLU(x)\n",
    "        x = self.oneone(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.oneone(x)\n",
    "        x = self.LogSoftmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = CausalConv1d(1, 1, 2, dilation = 512)\n",
    "        self.conv2 = CausalConv1d(1, 1, 2, dilation = 256)\n",
    "        self.conv3 = CausalConv1d(1, 1, 2, dilation = 128)\n",
    "        self.conv4 = CausalConv1d(1, 1, 2, dilation = 64)\n",
    "        self.conv5 = CausalConv1d(1, 1, 2, dilation = 32)\n",
    "        self.conv6 = CausalConv1d(1, 1, 2, dilation = 16)\n",
    "        self.conv7 = CausalConv1d(1, 1, 2, dilation = 8)\n",
    "        self.conv8 = CausalConv1d(1, 1, 2, dilation = 4)\n",
    "        self.conv9 = CausalConv1d(1, 1, 2, dilation = 2)\n",
    "        self.conv10 = CausalConv1d(1, 1, 2, dilation = 1)\n",
    "        self.Tanh = nn.Tanh()\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.oneone = nn.Conv1d(1, 1, 1)\n",
    "        self.LogSoftmax = nn.LogSoftmax(1)\n",
    "    \n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        def CreateNetwork(x1, one):\n",
    "            t1 = self.Tanh(x1)\n",
    "            s1 = self.Sigmoid(x1)\n",
    "            oneone = self.oneone(t1 * s1)\n",
    "            resAdd = oneone + one\n",
    "\n",
    "            return resAdd, oneone\n",
    "        \n",
    "        x1 = self.conv10(x)\n",
    "      \n",
    "        x2 = self.conv9(x1)\n",
    "        res1, one1 = CreateNetwork(x2, x1)\n",
    "        x3 = self.conv8(res1)\n",
    "        res2, one2 = CreateNetwork(x3, one1)\n",
    "        x4 = self.conv7(res2)\n",
    "        res3, one3 = CreateNetwork(x4, one2)\n",
    "        x5 = self.conv6(res3)\n",
    "        res4, one4 = CreateNetwork(x5, one3)\n",
    "        x6 = self.conv5(res4)\n",
    "        res5, one5 = CreateNetwork(x6, one4)\n",
    "        x7 = self.conv4(res5)\n",
    "        res6, one6 = CreateNetwork(x7, one5)\n",
    "        x8 = self.conv3(res6)\n",
    "        res7, one7 = CreateNetwork(x8, one6)\n",
    "        x9 = self.conv2(res7)\n",
    "        res8, one8 = CreateNetwork(x9, one7)\n",
    "        x10 = self.conv1(res8)\n",
    "        _, one9 = CreateNetwork(x10, one8)\n",
    "        \n",
    "        x = one1 + one2 + one3 + one4 + one5 + one6 + one7 + one8 + one9\n",
    "\n",
    "        x = self.ReLU(x)\n",
    "        x = self.oneone(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.oneone(x)\n",
    "        x = self.LogSoftmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Network = Net()\n",
    "\n",
    "if (torch.cuda.is_available() and CUDA):\n",
    "    Network.cuda()\n",
    "    print('CUDA Enabled; Running on GPU')\n",
    "    \n",
    "print(Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator():\n",
    "    while True:\n",
    "        xs = np.linspace(0, np.pi*10, 4000)\n",
    "        ys = np.sin(xs) + np.random.rand(4000)*0.2-0.1\n",
    "        ys = ys.astype(np.float32) # default is np.float64\n",
    "        x = torch.from_numpy(ys[:3999]).view(1,1,-1)\n",
    "        y = torch.from_numpy(ys[1:4000]).view(1,1,-1)\n",
    "        if (torch.cuda.is_available() and CUDA):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            \n",
    "        yield Variable(x), Variable(y)\n",
    "\n",
    "g = batch_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x, _ = next(g)\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(x.cpu().data.numpy()[0][0],'.', ms=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 10000\n",
    "# lr = 2e-3\n",
    "loss_save = []\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(Network.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    inputs, target = next(g)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = Network(inputs)\n",
    "    \n",
    "    loss = ((output-target)**2).mean() #criterion(output, target)\n",
    "    loss.backward()\n",
    "#     torch.nn.utils.clip_grad_norm(Network.parameters(), 0.002)\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_save.append(loss.data[0])\n",
    "    if epoch%1000 == 0:\n",
    "        print('epoch {}, loss {:3.4g}'.format(epoch, loss.data[0]))\n",
    "        \n",
    "#     if epoch%15000 == 0:\n",
    "#         lr *= 0.1\n",
    "#         optimizer = optim.Adam(Network.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(np.convolve(loss_save, np.ones(10)/10))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('averaged loss')\n",
    "plt.xlim([0,10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict from test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = next(g)\n",
    "x = x.cpu().data.numpy()\n",
    "n = 5000\n",
    "\n",
    "predict_save = []\n",
    "for _ in range(n):\n",
    "    inputs = Variable(torch.from_numpy(x))\n",
    "    \n",
    "    if (torch.cuda.is_available() and CUDA):\n",
    "        inputs = inputs.type(torch.cuda.FloatTensor)\n",
    "    \n",
    "    predict = Network(inputs)\n",
    "    \n",
    "    predict_save.append(predict.data[0][0][-1])\n",
    "    x = np.roll(x, -1, axis=2)\n",
    "    x[0][0][-1] = predict.data[0][0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(predict_save)\n",
    "plt.xlim([0, 5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict from random input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(1,1,1000).astype(np.float32)\n",
    "n = 5000\n",
    "\n",
    "predict_save = []\n",
    "for _ in range(n):\n",
    "    inputs = Variable(torch.from_numpy(x))\n",
    "    \n",
    "    if (torch.cuda.is_available() and CUDA):\n",
    "        inputs = inputs.type(torch.cuda.FloatTensor)\n",
    "    \n",
    "    predict = Network(inputs)\n",
    "    predict_save.append(predict.data[0][0][-1])\n",
    "    x = np.roll(x, -1, axis=2)\n",
    "    x[0][0][-1] = predict.data[0][0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(predict_save)\n",
    "plt.xlim([0,5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Network Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dot(var, params=None):\n",
    "    \"\"\" Produces Graphviz representation of PyTorch autograd graph\n",
    "    Blue nodes are the Variables that require grad, orange are Tensors\n",
    "    saved for backward in torch.autograd.Function\n",
    "    Args:\n",
    "        var: output Variable\n",
    "        params: dict of (name, Variable) to add names to node that\n",
    "            require grad (TODO: make optional)\n",
    "    \"\"\"\n",
    "    if params is not None:\n",
    "        assert isinstance(params.values()[0], Variable)\n",
    "        param_map = {id(v): k for k, v in params.items()}\n",
    "\n",
    "    node_attr = dict(style='filled',\n",
    "                     shape='box',\n",
    "                     align='left',\n",
    "                     fontsize='12',\n",
    "                     ranksep='0.1',\n",
    "                     height='0.2')\n",
    "    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
    "    seen = set()\n",
    "\n",
    "    def size_to_str(size):\n",
    "        return '('+(', ').join(['%d' % v for v in size])+')'\n",
    "\n",
    "    def add_nodes(var):\n",
    "        if var not in seen:\n",
    "            if torch.is_tensor(var):\n",
    "                dot.node(str(id(var)), size_to_str(var.size()), fillcolor='orange')\n",
    "            elif hasattr(var, 'variable'):\n",
    "                u = var.variable\n",
    "                name = param_map[id(u)] if params is not None else ''\n",
    "                node_name = '%s\\n %s' % (name, size_to_str(u.size()))\n",
    "                dot.node(str(id(var)), node_name, fillcolor='lightblue')\n",
    "            else:\n",
    "                dot.node(str(id(var)), str(type(var).__name__))\n",
    "            seen.add(var)\n",
    "            if hasattr(var, 'next_functions'):\n",
    "                for u in var.next_functions:\n",
    "                    if u[0] is not None:\n",
    "                        dot.edge(str(id(u[0])), str(id(var)))\n",
    "                        add_nodes(u[0])\n",
    "            if hasattr(var, 'saved_tensors'):\n",
    "                for t in var.saved_tensors:\n",
    "                    dot.edge(str(id(t)), str(id(var)))\n",
    "                    add_nodes(t)\n",
    "    add_nodes(var.grad_fn)\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, _ = next(g)\n",
    "x = x.cpu().data.numpy()\n",
    "\n",
    "inputs = Variable(torch.from_numpy(x))\n",
    "\n",
    "if (torch.cuda.is_available() and CUDA):\n",
    "    inputs = inputs.type(torch.cuda.FloatTensor)\n",
    "    \n",
    "out = Network(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = make_dot(out)\n",
    "graph.view()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
